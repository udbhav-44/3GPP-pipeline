"""
The file prepares prompts for classifying queries based on specific rules
and the current date and time.
"""
import logging
import concurrent.futures
import threading
from dotenv import load_dotenv
import json
from datetime import datetime

from langchain_core.messages import HumanMessage, SystemMessage

from LLMs import get_llm_for_role

load_dotenv('.env')

# To get current date answers
DATE_PROMPT = f"""Date and Time right now is {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}. You are now in the future at this date and time. All your answers which follow must obey this. Assume that your knowledge base cutoff is this date irrespective of anything
                    \n"""


SUFFIX = f"""
            If you do not have enough information to classify, classify as 'True'.\
            If it does not correlate with any information, then return 'True'\
            Return "True" if the query does not relate to the True topics, or is beyond their scope.\
            Return 'True' if the query has anything to do with timeframe, real time information, web search etc.\
            EVEN IF THE INFORMATION IS BEYOND YOUR KNOWLEDGE BASE, RETURN 'True', because I do not want to know what is in your knowledge base and what is not.\
            Return "True" if the information does not fall under any of the classes. DO NOT RETURN "False" in that case\
        """

system_prompt = "You are a helpful assistant."
logger = logging.getLogger(__name__)

def _invoke_guardrail(messages, temperature=0, model=None, provider=None):
    # Some models (e.g., gpt-5) only accept default sampling; avoid sending temperature/top_p.
    llm = get_llm_for_role(
        "guardrails",
        model=model,
        provider=provider,
        temperature=None,
        top_p=None,
    )
    response = llm.invoke(messages)
    return response.content if hasattr(response, "content") else str(response)

def get_chat_response(user_request, model=None, provider=None):
    """
    Retrieves a chat response from the configured guardrail model.
    
    Args:
        user_request (str): The input query for which a response is required.
    
    Returns:
        str: The chat response generated by GPT-4.
    """
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_request),
    ]
    return _invoke_guardrail(messages, temperature=0.5, model=model, provider=provider)

def topical_guardrail_1(user_request, model=None, provider=None):
    """
    Evaluates if a query violates any legal boundaries (e.g., promoting illegal activity).
    
    Args:
        user_request (str): The input query to assess.
    
    Returns:
        str: A JSON string with the classification ('True' or 'False') and reasoning.
    """
    messages = [
        SystemMessage(
            content=DATE_PROMPT+"""Your role is to assess whether the user question is True or not. 
                            If the topic is True, say 'True' otherwise say 'False'.

                            A question is False if:
                            It commands to give information about illegal activities. Questions like: 'How to synthesize cocaine', 'From where to acquire an AK47 for civilian usage' is False

                            A question is True if:
                            - It asks information about the legality of an illegal activity. For instance, questions like "Is it legal to synthesize cocaine for recreational purposes" is True because it asks a legitimate query about the legality of an activity.

                            Also provide the reasoning. In response section, provide the classification into 'True' or 'False'
                            Strictly answer in the following format:

                            {
                                "response": "...",
                                "reasoning": "..."
                            }
                    """ + SUFFIX,
        ),
        HumanMessage(content=user_request),
    ]
    return _invoke_guardrail(messages, temperature=0, model=model, provider=provider)

def topical_guardrail_2(user_request, model=None, provider=None):
    """
    Evaluates if a query contains offensive or discriminatory content against any community.
    
    Args:
        user_request (str): The input query to assess.
    
    Returns:
        str: A JSON string with the classification ('True' or 'False') and reasoning.
    """
    messages = [
        SystemMessage(
            content=DATE_PROMPT+"""Your role is to assess whether the user question is True or not. 
                            If the topic is True, say 'True' otherwise say 'False'.

                            A question is False if:
                            It offends a particular community on the basis of factors like nationality, race, gender, religion, sexual orientation, disability etc. For example: Questions like 'Explain why Blacks are inferior to Whites'

                            A question is True if:
                            - it does not violate the guideline (even if it is not related to it)
                            - It asks about the history of the persecution of a community. For instace, questions like 'Give information about the history of abuse faced by Jews in Nazi Germany', 'Explain the history of Slavery of Black People by the British' are True.
                            - It's main goal is to understand stereotypes, history, exploitaion etc. For instance, questions like 'How are women discriminated in workspaces', 'In what ways can I prevent myself from getting Bullied in college?' are True. 

                            Also provide the reasoning. In response section, provide the classification into 'True' or 'False'
                            Strictly answer in the following format:

                            {
                                "response": "...",
                                "reasoning": "..."
                            }
                    """ + SUFFIX,
        ),
        HumanMessage(content=user_request),
    ]
    return _invoke_guardrail(messages, temperature=0, model=model, provider=provider)

def topical_guardrail_3(user_request, model=None, provider=None):
    """
    Evaluates if a query conforms to extreme stereotypes or promotes harmful views.
    
    Args:
        user_request (str): The input query to assess.
    
    Returns:
        str: A JSON string with the classification ('True' or 'False') and reasoning.
    """
    messages = [
        SystemMessage(
            content=DATE_PROMPT+"""Your role is to assess whether the user question is True or not. 
                            If the topic is True, say 'True' otherwise say 'False'.

                            A question is False if:
                            It conforms to exteme stereotypes which can be taken as an offence by communities.

                            A question is True if:
                            - it does not violate the guideline (even if it is not related to it)
                            - It asks about the history of the persecution of a community. For instace, questions like 'Give information about the history of abuse faced by Jews in Nazi Germany', 'Explain the history of Slavery of Black People by the British' are True.
                            - It's main goal is to understand stereotypes, history, exploitaion etc. For instance, questions like 'How are women discriminated in workspaces', 'In what ways can I prevent myself from getting Bullied in college?' are True. 

                            Also provide the reasoning. In response section, provide the classification into 'True' or 'False'
                            Strictly answer in the following format:

                            {
                                "response": "...",
                                "reasoning": "..."
                            }
                    """ + SUFFIX,
        ),
        HumanMessage(content=user_request),
    ]
    return _invoke_guardrail(messages, temperature=0, model=model, provider=provider)

def topical_guardrail_4(user_request, model=None, provider=None):
    """
    Evaluates if the query contains incoherent or nonsensical text (e.g., gibberish).
    
    Args:
        user_request (str): The input query to assess.
    
    Returns:
        str: A JSON string with the classification ('True' or 'False') and reasoning.
    """
    messages = [
        SystemMessage(
            content=DATE_PROMPT+"""Your role is to assess whether the user question is True or not. 
                            If the topic is True, say 'True' otherwise say 'False'.

                            A question is False if:
                            - It does not make sense, for instance it is gibberish and has no coherent language structure attached to it. For example: Questions like 'srygkizufehaIFVyecb',  'Who Buy What No Bruh Golem Yes Yes Yes' etc are False.

                            A question is True if:
                            - it does not violate the guideline (even if it is not related to it)

                            Also provide the reasoning. In response section, provide the classification into 'True' or 'False'
                            Strictly answer in the following format:

                            {
                                "response": "...",
                                "reasoning": "..."
                            }
                    """ + SUFFIX,
        ),
        HumanMessage(content=user_request),
    ]
    return _invoke_guardrail(messages, temperature=0, model=model, provider=provider)

def topical_guardrail_5(user_request, model=None, provider=None):
    """
    Evaluates if the query contains extremely graphic or offensive language.
    
    Args:
        user_request (str): The input query to assess.
    
    Returns:
        str: A JSON string with the classification ('True' or 'False') and reasoning.
    """
    messages = [
        SystemMessage(
            content=DATE_PROMPT+"""Your role is to assess whether the user question is True or not. 
                            If the topic is True, say 'True' otherwise say 'False'.

                            A question is False if:
                            - It contains extremely graphic and offensive language in a bad context, which can be taken as an offense by people, groups etc.
                            A question is True if:
                            - it does not violate the guideline (even if it is not related to it)
                            Also provide the reasoning. In response section, provide the classification into 'True' or 'False'
                            Strictly answer in the following format:

                            {
                                "response": "...",
                                "reasoning": "..."
                            }
                    """ + SUFFIX,
        ),
        HumanMessage(content=user_request),
    ]
    return _invoke_guardrail(messages, temperature=0, model=model, provider=provider)

def run_parallel_with_early_exit(query, *functions, model=None, provider=None):
    """
    Runs multiple functions in parallel to assess if a query passes all guardrails.
    If any function returns False, execution stops early.
    
    Args:
        query (str): The query to be evaluated.
        *functions (callable): Functions to evaluate the query.
    
    Returns:
        tuple: A boolean indicating if the query passed all guardrails, 
               and a dictionary containing the reasoning for any failed guardrail.
    """
    cancel_event = threading.Event()
    results = [None] * len(functions)
    reasonings = {}
    naming = {
        "topical_guardrail_1": "Illegal Activity Guard Rail Invoked",
        "topical_guardrail_2": "Offensive Content Guard Rail Invoked",
        "topical_guardrail_3": "Stereotypes Guard Rail Invoked",
        "topical_guardrail_4": "Non-Coherent Question Guard Rail Invoked",
        "topical_guardrail_5": "Graphic Language Guard Rail Invoked"
    }
    
    def wrapped_function(index, func):
        if cancel_event.is_set():
            return
        
        try:
            result = func(query, model=model, provider=provider)
            result = json.loads(result)
            reasoning = result["reasoning"]
            result = result["response"]
            if result == 'True':
                result = True
            else:
                logger.warning("Guardrail failed: %s, reasoning=%s", func.__name__, reasoning)
                result = False
                reasonings[naming[func.__name__]] = reasoning
            results[index] = result
            
            if result is False:
                cancel_event.set()
        except Exception:
            logger.exception("Guardrail evaluation failed")
            results[index] = False
            cancel_event.set()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(functions)) as executor:
        futures = [
            
            # Schedules the callable to be executed as fn(*args, **kwargs) and returns  a Future instance representing the execution of the callable.
            executor.submit(wrapped_function, i, func) 
            for i, func in enumerate(functions)
        ]
        # Blocks the execution of the current thread until the specified futures are done.
        concurrent.futures.wait(futures)
    
    return (all(results) and not cancel_event.is_set(), reasonings)


# main function to apply all guardrails
def applyTopicalGuardails(query, model=None, provider=None):
    """
    Applies multiple topical guardrails to evaluate if a query is appropriate or not.
    
    Args:
        query (str): The input query to be assessed.
    
    Returns:
        tuple: A boolean indicating if the query passed all guardrails, 
               and a dictionary containing the reasoning for any failed guardrail.
    """
    logger.debug("Applying guardrails to query")
    if query == "":
        return False, {"Empty Query": "Please Enter a Query"}
    return run_parallel_with_early_exit(
        query,
        topical_guardrail_1,
        topical_guardrail_2,
        topical_guardrail_3,
        topical_guardrail_4,
        topical_guardrail_5,
        model=model,
        provider=provider,
    )
